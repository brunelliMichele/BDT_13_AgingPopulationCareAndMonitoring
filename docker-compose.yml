services:
    zookeeper:
        image: confluentinc/cp-zookeeper:7.4.0
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
        restart: always

    kafka:
        image: confluentinc/cp-kafka:7.4.0
        ports:
            - "9092:9092"
        depends_on:
            - zookeeper
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        restart: always

    spark:
        image: bitnami/spark:latest
        depends_on:
            - kafka
        volumes:
            - ./spark:/spark
            - ./spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
        command: >
                bash -c "sleep 10 &&
                /opt/bitnami/spark/bin/spark-submit
                --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0
                /spark/spark_stream.py"
        restart: always

    injector:
        image: python:3.10-slim
        depends_on:
            - kafka
        volumes:
            - ./kafka:/BDT_13/kafka
            - ./data:/BDT_13/data
        working_dir: /BDT_13/kafka
        command: bash -c "pip install -r requirements.txt && python3 injection.py"
    
    db:
        image: postgres:14.5
        environment:
            POSTGRES_USER: user
            POSTGRES_PASSWORD: password
            POSTGRES_DB: mydb
        ports: 
            - "5432:5432"
        volumes:
            - pgdata:/var/lib/postgresql/data
            - ./database/init:/docker-entrypoint-initdb.d
            - ./datbase/data:/data
        restart: always

volumes:
    pgdata: